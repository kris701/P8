\documentclass[../../main.tex]{subfiles}
\twocolumn
\begin{document}
\footnotesize

\section{Method}
\subsection{Feature Extraction}
The method used for feature extraction is a simple variation of the paper (insert shapelet paper here), combined with (insert BOSS paper here). It takes the idea of finding shapelets from information gain from the shapelet paper, and generates a histogram akin to the BOSS paper as output.

This section is for a simple modification of the shapelet paper, as it conveys the idea of the algorithm. However, changing it to output a histogram is a trivial modification.

In the following algortihm it is shown how to find the optimal shapelet.

\begin{algorithm}
\caption{FindingBestShapelet(data)}
\begin{algorithmic}
\Require $data = \{(class, series)\}$
\State $windows \gets Windows(data)$ \Comment{All windows from all series, for a given length}
\State $bestShapelet \gets 0$
\State $bestScore \gets -\infty$
\For{$w \in windows$}
	\State $score \gets ShapeletScore(data, w)$
	\If{$score > bestScore$}
		\State $bestShapelet \gets w$
		\State $bestScore \gets score$
	\EndIf
\EndFor

\Return $bestShaplet$
\end{algorithmic}
\end{algorithm}

It does so by evaluating all possible windows of a given size. This size is up to debate. The original paper gives a range, and therefore, considers all windows possible for all values in the given range. Another option, if the desired output is multiple shapelets, is to run the algorithm multiple times with different window sizes (or ranges).

\subsubsection{Shapelet Evaluation}
Evaluating a shapelet, is determining how much information is gained by splitting the dataset by it. The attribute of the split in question, is the frequency at which the shapelet occurs in a given series.
\begin{algorithm}
\caption{ShapeletScore(data, window)}
\begin{algorithmic}
\Require $data = \{(class, series)\}$
\State $points \gets {(class, 0)}$
\For{$d \in data$}
	\State $freq \gets MatchFrequency(d.seres, window)$
	\State $points \gets points \cup (d.class, freq)$
\EndFor

\Return $InformationGain(points)$
\end{algorithmic}
\end{algorithm}

The algorithm above, shows how to determing the score for a given shapelet(Simply another word for a window). It does so by finding the match frequency of all datapoints, I.e. each series tagged with a class, and then calculating the information gain possible for the given frequencies.

Information gain is calculated as the entropy decrease upon splitting the given points by their frequency. The following algorithm shows this,

\begin{algorithm}
\caption{InformationGain(points)}
\begin{algorithmic}
\Require $points = \{(class, frequency)\}$
\State $splitPoint \gets OptimalSplitPoint(points)$
\State $lower \gets \emptyset, higher \gets \emptyset$
\For{$p \in points$}
	\If{$p.frequency < splitPoint$}
		\State $lower \gets lower \cup p.class$
	\Else
		\State $higher \gets higher \cup p.class$
	\EndIf
\EndFor

\Return $H(points) - H(new)$
\end{algorithmic}
\end{algorithm}

As can as be seen, it first finds the optimal split point, which is the frequency that creates the highest divergence. (I don't know if this is the correct term) I.e. for each class best splits them into either of two groups. It then calculates, as seen in the last line, the information gain. This is simply how much the entropy of the original points is increased after the split.

In terms of finding the optimal split, it is quite simple, as it can be any point between the minimum and maximum frequency of the points. However, as every value between two points creates the same split. E.g. (1, 2, 3, 4) => split(2.1) = split(2.9), with (1, 2) lower, and (3, 4) higher. As such, one can simply check the midpoint - or any point - between all touching pairs. With a touching pair being the two values of the set of cloest value. This is shown in the algorithm below.

\begin{algorithm}
\caption{OptimalSplitPoint(points)}
\begin{algorithmic}
\Require $points = \{(class, frequency)\}$
\State $bestPoint \gets 0$
\State $bestScore \gets -\infty$
\For{$p \in possibleSplits$}
	\State $score \gets SplitScore(points, p)$
	\If{$score > bestScore$}
		\State $bestPoint \gets p$
		\State $bestScore \gets score$ 
	\EndIf
\EndFor

\Return $bestPoint$
\end{algorithmic}
\end{algorithm}

Finding the split score is relatively simple. In essence it splits all points into two groups, one where all frequencies are higher than the splitPoint, and another where they are lower. It then simply takes the sum of the difference in member count of each class per group. E.g. Group 1 (A, A, B), Group 2 (B, C) $\rightarrow$ A = |2 - 0| = 2, B = |1 - 1| = 0, C = |0 - 1| = 1. A modification can be seen in algorithm below.

\begin{algorithm}
\caption{SplitScore(points, splitPoint)}
\begin{algorithmic}
\Require $points = \{(class, frequency)\}$
\State $scores \gets \{(class, 0)\}$
\For{$p \in points$}
		\If{$p.frequency < splitPoint$}
			\State $scores[p.class] \gets scores(p.class) + 1$
		\Else{}
			\State $scores[p.class] \gets scores(p.class) - 1$
		\EndIf
\EndFor

\Return $\sum_{s \in scores} |s|$ \Comment{For every score}
\end{algorithmic}
\end{algorithm}

It does it by substituing the groups by a number, where positive increments indicates membership for the given class in group A, while negative increments for group B. The absolute value of the number is then the difference in membership count for the two groups.

In an actual implementation, this could be pulled out to the information gain algorithm, as it recalculates the difference in membership count.

\subsubsection{Match Frequency}
In essence the match frequency can be thought of as sliding the window across the given series, and counting how many times the two aligns. This is shown in the below algorithm.
\begin{algorithm}
\caption{MatchFrequency(series, window)}
\begin{algorithmic}
\State $matchCount \gets 0$
\For{$w \in Windows(series)$}
	\If{$IsMatch(w, window)$}
		\State $matchCount \gets matchCount + 1$
	\EndIf
\EndFor

\Return $matchCount$
\end{algorithmic}
\end{algorithm}

However, the more interesting part is checking whether the two aligns. As there a lot of different ways to do it, with even the possibility of handling noise.

The naive method, checking euclidian distance between the points of the series, and returning false if a single of them is greater than zero. I.e. the two align perfectly. Happens rather infrequently, as such a simple improvement is to add a tolerance, which can be seen in the following algorithm.\\\\

\begin{algorithm}
\caption{IsMatch(series1, series2)}
\begin{algorithmic}
\Require $tolerance \gets 0.01$ \Comment{A parameter specifying the fussyness of the match}
\If{$|series1| \neq |series2|$}

	\Return $False$
	
\EndIf

\State $offset \gets series2[0] - series1[0]$
\For{$i \gets 0$ to $|series1|$}
	\State $dist \gets abs( series1[i] - series2[i] - offset )$
	\If{$dist > tolerance$}
	
		\Return $False$
		
	\EndIf
\EndFor

\Return $True$
\end{algorithmic}
\end{algorithm}

The algorithm simply iterates over each point in both series, assuming they are of the same length, and checks whether the distance between the series is at any point above the tolerance. However, even this small tolerance parameter could be either absolute, as seen in the algorithm, or relative to value of the given series.

One could also allow a certain number of misses, I.e. if n-1 points match it is still a match. Or change the distance measure, e.g. to be the average of the point and it's neighbours. And so on.

\end{document}