@article{inpr_DBLP2020,
  author    = {Wensi Tang and
               Lu Liu and
               Guodong Long},
  title     = {Interpretable Time-series Classification on Few-shot Samples},
  journal   = {CoRR},
  volume    = {abs/2006.02031},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.02031},
  eprinttype = {arXiv},
  eprint    = {2006.02031},
  timestamp = {Tue, 06 Dec 2022 08:39:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-02031.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{inpr_fisher2018,
author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
year = {2018},
month = {01},
pages = {},
title = {Model Class Reliance: Variable Importance Measures for any Machine Learning Model Class, from the "Rashomon" Perspective}
}

@inproceedings{inpr_mmd-critic2016,
 author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Examples are not enough, learn to criticize! Criticism for Interpretability},
 url = {https://proceedings.neurips.cc/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf},
 volume = {29},
 year = {2016}
}

@article{inpr_lrp2018,
title = {Methods for interpreting and understanding deep neural networks},
journal = {Digital Signal Processing},
volume = {73},
pages = {1-15},
year = {2018},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2017.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S1051200417302385},
author = {Grégoire Montavon and Wojciech Samek and Klaus-Robert Müller},
keywords = {Deep neural networks, Activation maximization, Sensitivity analysis, Taylor decomposition, Layer-wise relevance propagation},
abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.}
}

@article{inpr_attention2021,
title = {Interpretability of time-series deep learning models: A study in cardiovascular patients admitted to Intensive care unit},
journal = {Journal of Biomedical Informatics},
volume = {121},
pages = {103876},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103876},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421002057},
author = {Ilaria Gandin and Arjuna Scagnetto and Simona Romani and Giulia Barbati},
keywords = {Deep learning, Interpretability, Electronic health records},
}

@ARTICLE{inpr_pip2022,
  author={Ghods, Alireza and Cook, Diane J.},
  journal={IEEE Computational Intelligence Magazine}, 
  title={PIP: Pictorial Interpretable Prototype Learning for Time Series Classification}, 
  year={2022},
  volume={17},
  number={1},
  pages={34-45},
  doi={10.1109/MCI.2021.3129957}}

@INPROCEEDINGS{inpr_sax-vsm2013,
  author={Senin, Pavel and Malinchik, Sergey},
  booktitle={2013 IEEE 13th International Conference on Data Mining}, 
  title={SAX-VSM: Interpretable Time Series Classification Using SAX and Vector Space Model}, 
  year={2013},
  volume={},
  number={},
  pages={1175-1180},
  doi={10.1109/ICDM.2013.52}}

@article{proto2017,
  author    = {Jake Snell and
               Kevin Swersky and
               Richard S. Zemel},
  title     = {Prototypical Networks for Few-shot Learning},
  journal   = {CoRR},
  volume    = {abs/1703.05175},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.05175},
  eprinttype = {arXiv},
  eprint    = {1703.05175},
  timestamp = {Mon, 13 Aug 2018 16:46:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SnellSZ17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{narwariya2020,
author = {Narwariya, Jyoti and Malhotra, Pankaj and Vig, Lovekesh and Shroff, Gautam and Vishnu, T. V.},
title = {Meta-Learning for Few-Shot Time Series Classification},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371162},
doi = {10.1145/3371158.3371162},
abstract = {Deep neural networks (DNNs) have achieved state-of-the-art results on time series classification (TSC) tasks. In this work, we focus on leveraging DNNs in the often-encountered practical scenario where access to labeled training data is difficult, and where DNNs would be prone to overfitting. We leverage recent advancements in gradient-based meta-learning, and propose an approach to train a residual neural network with convolutional layers as a meta-learning agent for few-shot TSC. The network is trained on a diverse set of few-shot tasks sampled from various domains (e.g. healthcare, activity recognition, etc.) such that it can solve a target task from another domain using only a small number of training samples from the target task. Most existing meta-learning approaches are limited in practice as they assume a fixed number of target classes across tasks. We overcome this limitation in order to train a common agent across domains with each domain having different number of target classes, we utilize a triplet-loss based learning procedure that does not require any constraints to be enforced on the number of classes for the few-shot TSC tasks. To the best of our knowledge, we are the first to use meta-learning based pre-training for TSC. Our approach sets a new benchmark for few-shot TSC, outperforming several strong baselines on few-shot tasks sampled from 41 datasets in UCR TSC Archive. We observe that pre-training under the meta-learning paradigm allows the network to quickly adapt to new unseen tasks with small number of labeled instances.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {28–36},
numpages = {9},
keywords = {Few-Shot Learning, Meta-Learning, Convolutional Neural Networks, Time Series Classification},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}
